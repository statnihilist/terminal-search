#!/usr/bin/env python3
#This is a simple webscraper to scrap summary of any topic from
#https://en.wikipedia.org


# importing modules
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support.expected_conditions import staleness_of
from selenium.webdriver.common.keys import Keys
import sys
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse


# ---------------------------------------------------------
# handles the browser

# setting up a headless browser
def options(flag):
    if (flag == 'firefox'):
        from selenium.webdriver.firefox.options import Options
    elif (flag == 'chrome'):
        from selenium.webdriver.chrome.options import Options

    options = Options()
    options.set_headless(headless = True)

    return options


# setting path to log unavoidable logs for firefox
def log_path():
    path_file = sys.argv[0]
    path_dir = '/'.join(path_file.split('/')[:-1])
    log_path = path_dir + '/log/geckodriver.log'

    return log_path


# choosing browser
def choose_browser():
    print('\nWelcome to Wikipedia-Search!')
    print('\nPlease choose your browser : ')
    print('1. Firefox')
    print('2. Chrome')
    print('\nChoice :', end = ' ')
    choice_input = input()
    while True:
        if (choice_input == '1' or choice_input.lower() == 'firefox'):
            browser = webdriver.Firefox(firefox_options = options('firefox'), \
            log_path = log_path())
            flag = 'firefox'
            return browser, flag
        elif (choice_input == '2' or choice_input.lower() == 'chrome'):
            browser = webdriver.Chrome(chrome_options = options('chrome'))
            flag = 'chrome'
            return browser, flag
        else:
            print('Invalid choice. Please try again! ')
            print('\nChoice :', end = ' ')
            choice_input = input()

# ---------------------------------------------------------
# handles the search

# handles the socket and returns soup
def socket_soup(searched_url):
    # opening a socket and ingnoring proxies
    session = requests.Session()
    session.trust_env = False
    source = session.get(searched_url)

    # reading the data in a variable
    html = source.text
    soup = BeautifulSoup(html, 'html.parser')

    return soup


# search results streamimg
def search_results(browser, searched_url):
    #finding appropriate section of the page
    soup = socket_soup(searched_url)
    header_para = soup.find('p', attrs = {'class' : 'mw-search-createlink'})
    print('\n' + header_para.text)

    try:
        results = []            # to store text results
        results_links = []      # to store corresponding partial link

        # finding all possible results on the page
        for i in range(20):
            content = soup.find('a', attrs = {'data-serp-pos' : i})
            results.append(content.text)
            results_links.append(content.get('href'))
        print('\nAvailable results:')
        for i, j in enumerate(results):
            print(str(i+1) + '.', j)

        # to custom search from the given results
        print('\nWhat would you like to search?(Enter a number from the list.)')
        print('Enter 0 if you want to search something not on the list.')
        print('Choice :', end = ' ')
        link_no = int(input()) - 1

        # generating complete url
        if(link_no != -1):
            partial_url = results_links[link_no]
            complete_url = 'https://en.wikipedia.org' + partial_url

            # finding appropriate section of the page
            soup = socket_soup(complete_url)
            req_para = soup.find_all('p')[0]
            print('\nSummary:\n', req_para.text)

        if(link_no == -1):
            custom_search(browser)
        else:
            pass
    except:
        print('There are no results matching the query.')
        custom_search(browser)

    return


# prints page content for custom search
def legit_page(searched_url):
    # finding appropriate section of the page
    soup = socket_soup(searched_url)
    req_para = soup.find_all('p')[0]
    print('\nSummary:\n', req_para.text)

    return


# top article streaming
def top_article(searched_url):
    # finding appropriate section of the pages
    soup = socket_soup(searched_url)
    req_para = soup.find_all('p')[0]
    print('\nFeatured Article:\n', req_para.text)

    return


# custom search
def custom_search(browser, search_input = ''):

    # searching in search box
    elem = browser.find_element(by = 'id', value = 'searchInput')
    if(search_input == ''):
        print('\nPlease specify search :', end = ' ')
        elem.send_keys(input() + Keys.RETURN)
    else:
        elem.send_keys(search_input + Keys.RETURN)

    # to handle loading of new pages in selenium
    # through the method of checking staleness of page
    # only for firefox
    if (choice_flag == 'firefox'):
        timeout = 30
        old_page = browser.find_element_by_tag_name('html')
        WebDriverWait(browser, timeout).until(staleness_of(old_page))
    else:
        pass

    # check if search is appropriate or not
    searched_url = browser.current_url
    parsed_url = urlparse(searched_url)
    if(parsed_url.path == '/w/index.php'):
        search_results(browser, searched_url)
    else:
        legit_page(searched_url)

    return


# what to be streamed for the user
def search_kind(browser, search_input = ''):
    if(search_input == ''):
        print('\nWhat would you like to know?')
        print("1. Today's featured article")
        print('2. Custom Search')
        print('\nChoice :', end = ' ')
        search_kind_input = input()
        while True:
            if(search_kind_input == '1'):
                top_article(browser.current_url)
                return
            elif(search_kind_input == '2'):
                custom_search(browser)
                return
            else:
                print('Invalid choice. Please try again!')
                print('Choice :', end = ' ')
                search_kind_input = input()
    else:
        custom_search(browser, search_input)

    return


# handling parameters obtained from terminal
def search(browser):
    if len(sys.argv) == 1:
        search_kind(browser)
    else:
        search_input = sys.argv[1:]
        search_input = ' '.join(search_input)
        search_kind(browser, search_input)

    return


# navigate to page via selenium
def searcher(choice, choice_flag):

    # opening site in web browser
    browser = choice
    browser.get('https://en.wikipedia.org')

    # searching for appropriate page and info
    search(browser)

    # closing browser
    browser.close()

    return

# ---------------------------------------------------------
# main

choice, choice_flag = choose_browser()
searcher(choice, choice_flag)
